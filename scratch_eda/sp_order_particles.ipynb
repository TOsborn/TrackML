{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to create tracks with ordered hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import abs, sqrt, udf\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"order_hits\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_pt_f(pT):\n",
    "    if pT <= 0.5:\n",
    "        return 0.2\n",
    "    if pT >= 3.0:\n",
    "        return 1.0\n",
    "    return 0.32*(pT - 0.5) + 0.2\n",
    "\n",
    "weight_pt = udf(weight_pt_f, DoubleType())\n",
    "\n",
    "def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/efs/dataset/train/event000001000-truth.csv\n",
      "/home/ec2-user/SageMaker/efs/dataset/blacklist/event000001000-blacklist-particles.csv\n",
      "/home/ec2-user/SageMaker/efs/dataset/blacklist/event000001000-blacklist-hits.csv\n"
     ]
    }
   ],
   "source": [
    "event_id = '000001000'\n",
    "\n",
    "truth_url = '/home/ec2-user/SageMaker/efs/dataset/train/event%s-truth.csv'\n",
    "blacklist_particles_url = '/home/ec2-user/SageMaker/efs/dataset/blacklist/event%s-blacklist-particles.csv'\n",
    "blacklist_hits_url = '/home/ec2-user/SageMaker/efs/dataset/blacklist/event%s-blacklist-hits.csv'\n",
    "\n",
    "print(truth_url %event_id)\n",
    "print(blacklist_particles_url %event_id)\n",
    "print(blacklist_hits_url %event_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(url, inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrt(df.tpx**2 + df.tpz**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('pT', sqrt(df.tpx**2 + df.tpy**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+--------+---------+-------+---------+----------+---------+----------+-------------------+\n",
      "|hit_id|       particle_id|      tx|       ty|     tz|      tpx|       tpy|      tpz|    weight|                 pT|\n",
      "+------+------------------+--------+---------+-------+---------+----------+---------+----------+-------------------+\n",
      "|     1|                 0|-64.4116| -7.16412|-1502.5| 250710.0| -149908.0|-956385.0|       0.0| 292109.41882109863|\n",
      "|     2| 22525763437723648|-55.3385| 0.630805|-1502.5|-0.570605| 0.0283904| -15.4922|9.86408E-6| 0.5713108443195876|\n",
      "|     3|                 0| -83.828| -1.14558|-1502.5| 626295.0| -169767.0|-760877.0|       0.0|   648896.186854261|\n",
      "|     4|297237712845406208|-96.1229| -8.23036|-1502.5|-0.225235|-0.0509684| -3.70232|8.13311E-6| 0.2309298227244805|\n",
      "|     5|418835796137607168|-62.6594| -9.37504|-1502.5|-0.281806| -0.023487| -6.57318|8.87338E-6| 0.2827830631508896|\n",
      "|     6|108087696726949888|-57.0856| -8.18971|-1502.5|-0.401129| -0.035276| -10.4669|8.13311E-6| 0.4026771297416828|\n",
      "|     7|968286151951515648|-73.8608| -2.57586|-1502.5|-0.442662|-0.0369686|  -9.1301|6.96788E-6| 0.4442030207348437|\n",
      "|     8|954766419537428480|-63.8512| -10.8754|-1502.5|-0.670459|-0.0926088| -15.5407|1.13644E-5| 0.6768246896489815|\n",
      "|     9|707072769359085568|-97.2489| -10.9067|-1502.5|-0.279789|-0.0621434| -4.41292|8.13311E-6|0.28660719928948053|\n",
      "|    10| 67554956483231744|-90.2763| -3.24397|-1502.5|-0.251752|-0.0371381| -4.24922|6.45466E-6|0.25447653718095503|\n",
      "|    11|274720539342274560|-59.1974|-0.647788|-1502.5|-0.208248| -0.018635|   -5.283|8.13311E-6|0.20908011079249023|\n",
      "|    12|                 0|-42.6177| -10.6661|-1502.5|-270443.0|   83056.7|-959147.0|       0.0| 282909.58213515853|\n",
      "|    13| 63061596057894912|-72.5146|  2.62395|-1502.5|-0.264801| 0.0295635| -5.52233|6.96788E-6|0.26644618618634797|\n",
      "|    14|707068508751527936|-63.5152| -1.85185|-1502.5|-0.218837|-0.0294531| -5.28927|8.13311E-6|0.22081013941531308|\n",
      "|    15| 63064276117487616|-63.4467| -3.24381|-1502.5|-0.203056| 0.0117957|   -4.807|6.96788E-6|0.20339832269340372|\n",
      "|    16| 67554612885848064|-57.3076| -5.08441|-1502.5|-0.205477| -0.038563|  -5.5643|6.96788E-6|  0.209064350136507|\n",
      "|    17|824159968759382016|-97.2745|  -6.5533|-1502.5|-0.452092|-0.0030149| -6.98815|8.13311E-6| 0.4521020527336831|\n",
      "|    18|103584646855393280|-79.9894| -7.17084|-1502.5|-0.854836| -0.100101| -16.1965|9.62518E-6| 0.8606769411904794|\n",
      "|    19|878204126360502272|-100.931| 0.397845|-1502.5|-0.642083|-0.0345362| -9.62226|7.96041E-6|  0.643011141427145|\n",
      "|    20|211686362355597312|-57.1352|   2.7655|-1502.5|-0.226328| 0.0289454|  -5.8074|7.47422E-6|0.22817142626797074|\n",
      "+------+------------------+--------+---------+-------+---------+----------+---------+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Column in module pyspark.sql.column object:\n",
      "\n",
      "class Column(builtins.object)\n",
      " |  A column in a DataFrame.\n",
      " |  \n",
      " |  :class:`Column` instances can be created by::\n",
      " |  \n",
      " |      # 1. Select a column out of a DataFrame\n",
      " |  \n",
      " |      df.colName\n",
      " |      df[\"colName\"]\n",
      " |  \n",
      " |      # 2. Create from an expression\n",
      " |      df.colName + 1\n",
      " |      1 / df.colName\n",
      " |  \n",
      " |  .. versionadded:: 1.3\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __and__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __bool__ = __nonzero__(self)\n",
      " |  \n",
      " |  __contains__(self, item)\n",
      " |      # container operators\n",
      " |  \n",
      " |  __div__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __eq__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __ge__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __getattr__(self, item)\n",
      " |  \n",
      " |  __getitem__(self, k)\n",
      " |  \n",
      " |  __gt__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __init__(self, jc)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __invert__ = _(self)\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |  \n",
      " |  __le__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __lt__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __mod__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __mul__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __ne__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __neg__ = _(self)\n",
      " |  \n",
      " |  __nonzero__(self)\n",
      " |  \n",
      " |  __or__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __pow__ = _(self, other)\n",
      " |      binary function\n",
      " |  \n",
      " |  __radd__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rand__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rdiv__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __rmod__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rmul__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __ror__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rpow__ = _(self, other)\n",
      " |      binary function\n",
      " |  \n",
      " |  __rsub__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rtruediv__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __sub__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __truediv__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  alias(self, *alias, **kwargs)\n",
      " |      Returns this column aliased with a new name or names (in the case of expressions that\n",
      " |      return more than one column, such as explode).\n",
      " |      \n",
      " |      :param alias: strings of desired column names (collects all positional arguments passed)\n",
      " |      :param metadata: a dict of information to be stored in ``metadata`` attribute of the\n",
      " |          corresponding :class: `StructField` (optional, keyword only argument)\n",
      " |      \n",
      " |      .. versionchanged:: 2.2\n",
      " |         Added optional ``metadata`` argument.\n",
      " |      \n",
      " |      >>> df.select(df.age.alias(\"age2\")).collect()\n",
      " |      [Row(age2=2), Row(age2=5)]\n",
      " |      >>> df.select(df.age.alias(\"age3\", metadata={'max': 99})).schema['age3'].metadata['max']\n",
      " |      99\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  asc = _(self)\n",
      " |      Returns a sort expression based on the ascending order of the given column name\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(name='Tom', height=80), Row(name='Alice', height=None)])\n",
      " |      >>> df.select(df.name).orderBy(df.name.asc()).collect()\n",
      " |      [Row(name='Alice'), Row(name='Tom')]\n",
      " |  \n",
      " |  astype = cast(self, dataType)\n",
      " |      :func:`astype` is an alias for :func:`cast`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  between(self, lowerBound, upperBound)\n",
      " |      A boolean expression that is evaluated to true if the value of this\n",
      " |      expression is between the given columns.\n",
      " |      \n",
      " |      >>> df.select(df.name, df.age.between(2, 4)).show()\n",
      " |      +-----+---------------------------+\n",
      " |      | name|((age >= 2) AND (age <= 4))|\n",
      " |      +-----+---------------------------+\n",
      " |      |Alice|                       true|\n",
      " |      |  Bob|                      false|\n",
      " |      +-----+---------------------------+\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  bitwiseAND = _(self, other)\n",
      " |      Compute bitwise AND of this expression with another expression.\n",
      " |      \n",
      " |      :param other: a value or :class:`Column` to calculate bitwise and(&) against\n",
      " |                    this :class:`Column`.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(a=170, b=75)])\n",
      " |      >>> df.select(df.a.bitwiseAND(df.b)).collect()\n",
      " |      [Row((a & b)=10)]\n",
      " |  \n",
      " |  bitwiseOR = _(self, other)\n",
      " |      Compute bitwise OR of this expression with another expression.\n",
      " |      \n",
      " |      :param other: a value or :class:`Column` to calculate bitwise or(|) against\n",
      " |                    this :class:`Column`.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(a=170, b=75)])\n",
      " |      >>> df.select(df.a.bitwiseOR(df.b)).collect()\n",
      " |      [Row((a | b)=235)]\n",
      " |  \n",
      " |  bitwiseXOR = _(self, other)\n",
      " |      Compute bitwise XOR of this expression with another expression.\n",
      " |      \n",
      " |      :param other: a value or :class:`Column` to calculate bitwise xor(^) against\n",
      " |                    this :class:`Column`.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(a=170, b=75)])\n",
      " |      >>> df.select(df.a.bitwiseXOR(df.b)).collect()\n",
      " |      [Row((a ^ b)=225)]\n",
      " |  \n",
      " |  cast(self, dataType)\n",
      " |      Convert the column into type ``dataType``.\n",
      " |      \n",
      " |      >>> df.select(df.age.cast(\"string\").alias('ages')).collect()\n",
      " |      [Row(ages='2'), Row(ages='5')]\n",
      " |      >>> df.select(df.age.cast(StringType()).alias('ages')).collect()\n",
      " |      [Row(ages='2'), Row(ages='5')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  contains = _(self, other)\n",
      " |      Contains the other element. Returns a boolean :class:`Column` based on a string match.\n",
      " |      \n",
      " |      :param other: string in line\n",
      " |      \n",
      " |      >>> df.filter(df.name.contains('o')).collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |  \n",
      " |  desc = _(self)\n",
      " |      Returns a sort expression based on the descending order of the given column name.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(name='Tom', height=80), Row(name='Alice', height=None)])\n",
      " |      >>> df.select(df.name).orderBy(df.name.desc()).collect()\n",
      " |      [Row(name='Tom'), Row(name='Alice')]\n",
      " |  \n",
      " |  endswith = _(self, other)\n",
      " |      String ends with. Returns a boolean :class:`Column` based on a string match.\n",
      " |      \n",
      " |      :param other: string at end of line (do not use a regex `$`)\n",
      " |      \n",
      " |      >>> df.filter(df.name.endswith('ice')).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      >>> df.filter(df.name.endswith('ice$')).collect()\n",
      " |      []\n",
      " |  \n",
      " |  eqNullSafe = _(self, other)\n",
      " |      Equality test that is safe for null values.\n",
      " |      \n",
      " |      :param other: a value or :class:`Column`\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df1 = spark.createDataFrame([\n",
      " |      ...     Row(id=1, value='foo'),\n",
      " |      ...     Row(id=2, value=None)\n",
      " |      ... ])\n",
      " |      >>> df1.select(\n",
      " |      ...     df1['value'] == 'foo',\n",
      " |      ...     df1['value'].eqNullSafe('foo'),\n",
      " |      ...     df1['value'].eqNullSafe(None)\n",
      " |      ... ).show()\n",
      " |      +-------------+---------------+----------------+\n",
      " |      |(value = foo)|(value <=> foo)|(value <=> NULL)|\n",
      " |      +-------------+---------------+----------------+\n",
      " |      |         true|           true|           false|\n",
      " |      |         null|          false|            true|\n",
      " |      +-------------+---------------+----------------+\n",
      " |      >>> df2 = spark.createDataFrame([\n",
      " |      ...     Row(value = 'bar'),\n",
      " |      ...     Row(value = None)\n",
      " |      ... ])\n",
      " |      >>> df1.join(df2, df1[\"value\"] == df2[\"value\"]).count()\n",
      " |      0\n",
      " |      >>> df1.join(df2, df1[\"value\"].eqNullSafe(df2[\"value\"])).count()\n",
      " |      1\n",
      " |      >>> df2 = spark.createDataFrame([\n",
      " |      ...     Row(id=1, value=float('NaN')),\n",
      " |      ...     Row(id=2, value=42.0),\n",
      " |      ...     Row(id=3, value=None)\n",
      " |      ... ])\n",
      " |      >>> df2.select(\n",
      " |      ...     df2['value'].eqNullSafe(None),\n",
      " |      ...     df2['value'].eqNullSafe(float('NaN')),\n",
      " |      ...     df2['value'].eqNullSafe(42.0)\n",
      " |      ... ).show()\n",
      " |      +----------------+---------------+----------------+\n",
      " |      |(value <=> NULL)|(value <=> NaN)|(value <=> 42.0)|\n",
      " |      +----------------+---------------+----------------+\n",
      " |      |           false|           true|           false|\n",
      " |      |           false|          false|            true|\n",
      " |      |            true|          false|           false|\n",
      " |      +----------------+---------------+----------------+\n",
      " |      \n",
      " |      .. note:: Unlike Pandas, PySpark doesn't consider NaN values to be NULL.\n",
      " |         See the `NaN Semantics`_ for details.\n",
      " |      .. _NaN Semantics:\n",
      " |         https://spark.apache.org/docs/latest/sql-programming-guide.html#nan-semantics\n",
      " |      .. versionadded:: 2.3.0\n",
      " |  \n",
      " |  getField(self, name)\n",
      " |      An expression that gets a field by name in a StructField.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(r=Row(a=1, b=\"b\"))])\n",
      " |      >>> df.select(df.r.getField(\"b\")).show()\n",
      " |      +---+\n",
      " |      |r.b|\n",
      " |      +---+\n",
      " |      |  b|\n",
      " |      +---+\n",
      " |      >>> df.select(df.r.a).show()\n",
      " |      +---+\n",
      " |      |r.a|\n",
      " |      +---+\n",
      " |      |  1|\n",
      " |      +---+\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  getItem(self, key)\n",
      " |      An expression that gets an item at position ``ordinal`` out of a list,\n",
      " |      or gets an item by key out of a dict.\n",
      " |      \n",
      " |      >>> df = spark.createDataFrame([([1, 2], {\"key\": \"value\"})], [\"l\", \"d\"])\n",
      " |      >>> df.select(df.l.getItem(0), df.d.getItem(\"key\")).show()\n",
      " |      +----+------+\n",
      " |      |l[0]|d[key]|\n",
      " |      +----+------+\n",
      " |      |   1| value|\n",
      " |      +----+------+\n",
      " |      >>> df.select(df.l[0], df.d[\"key\"]).show()\n",
      " |      +----+------+\n",
      " |      |l[0]|d[key]|\n",
      " |      +----+------+\n",
      " |      |   1| value|\n",
      " |      +----+------+\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  isNotNull = _(self)\n",
      " |      True if the current expression is NOT null.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(name='Tom', height=80), Row(name='Alice', height=None)])\n",
      " |      >>> df.filter(df.height.isNotNull()).collect()\n",
      " |      [Row(height=80, name='Tom')]\n",
      " |  \n",
      " |  isNull = _(self)\n",
      " |      True if the current expression is null.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(name='Tom', height=80), Row(name='Alice', height=None)])\n",
      " |      >>> df.filter(df.height.isNull()).collect()\n",
      " |      [Row(height=None, name='Alice')]\n",
      " |  \n",
      " |  isin(self, *cols)\n",
      " |      A boolean expression that is evaluated to true if the value of this\n",
      " |      expression is contained by the evaluated values of the arguments.\n",
      " |      \n",
      " |      >>> df[df.name.isin(\"Bob\", \"Mike\")].collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      >>> df[df.age.isin([1, 2, 3])].collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      \n",
      " |      .. versionadded:: 1.5\n",
      " |  \n",
      " |  like = _(self, other)\n",
      " |      SQL like expression. Returns a boolean :class:`Column` based on a SQL LIKE match.\n",
      " |      \n",
      " |      :param other: a SQL LIKE pattern\n",
      " |      \n",
      " |      See :func:`rlike` for a regex version\n",
      " |      \n",
      " |      >>> df.filter(df.name.like('Al%')).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |  \n",
      " |  name = alias(self, *alias, **kwargs)\n",
      " |      :func:`name` is an alias for :func:`alias`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  otherwise(self, value)\n",
      " |      Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      " |      If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n",
      " |      \n",
      " |      See :func:`pyspark.sql.functions.when` for example usage.\n",
      " |      \n",
      " |      :param value: a literal value, or a :class:`Column` expression.\n",
      " |      \n",
      " |      >>> from pyspark.sql import functions as F\n",
      " |      >>> df.select(df.name, F.when(df.age > 3, 1).otherwise(0)).show()\n",
      " |      +-----+-------------------------------------+\n",
      " |      | name|CASE WHEN (age > 3) THEN 1 ELSE 0 END|\n",
      " |      +-----+-------------------------------------+\n",
      " |      |Alice|                                    0|\n",
      " |      |  Bob|                                    1|\n",
      " |      +-----+-------------------------------------+\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  over(self, window)\n",
      " |      Define a windowing column.\n",
      " |      \n",
      " |      :param window: a :class:`WindowSpec`\n",
      " |      :return: a Column\n",
      " |      \n",
      " |      >>> from pyspark.sql import Window\n",
      " |      >>> window = Window.partitionBy(\"name\").orderBy(\"age\").rowsBetween(-1, 1)\n",
      " |      >>> from pyspark.sql.functions import rank, min\n",
      " |      >>> # df.select(rank().over(window), min('age').over(window))\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  rlike = _(self, other)\n",
      " |      SQL RLIKE expression (LIKE with Regex). Returns a boolean :class:`Column` based on a regex\n",
      " |      match.\n",
      " |      \n",
      " |      :param other: an extended regex expression\n",
      " |      \n",
      " |      >>> df.filter(df.name.rlike('ice$')).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |  \n",
      " |  startswith = _(self, other)\n",
      " |      String starts with. Returns a boolean :class:`Column` based on a string match.\n",
      " |      \n",
      " |      :param other: string at start of line (do not use a regex `^`)\n",
      " |      \n",
      " |      >>> df.filter(df.name.startswith('Al')).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      >>> df.filter(df.name.startswith('^Al')).collect()\n",
      " |      []\n",
      " |  \n",
      " |  substr(self, startPos, length)\n",
      " |      Return a :class:`Column` which is a substring of the column.\n",
      " |      \n",
      " |      :param startPos: start position (int or Column)\n",
      " |      :param length:  length of the substring (int or Column)\n",
      " |      \n",
      " |      >>> df.select(df.name.substr(1, 3).alias(\"col\")).collect()\n",
      " |      [Row(col='Ali'), Row(col='Bob')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  when(self, condition, value)\n",
      " |      Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      " |      If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n",
      " |      \n",
      " |      See :func:`pyspark.sql.functions.when` for example usage.\n",
      " |      \n",
      " |      :param condition: a boolean :class:`Column` expression.\n",
      " |      :param value: a literal value, or a :class:`Column` expression.\n",
      " |      \n",
      " |      >>> from pyspark.sql import functions as F\n",
      " |      >>> df.select(df.name, F.when(df.age > 4, 1).when(df.age < 3, -1).otherwise(0)).show()\n",
      " |      +-----+------------------------------------------------------------+\n",
      " |      | name|CASE WHEN (age > 4) THEN 1 WHEN (age < 3) THEN -1 ELSE 0 END|\n",
      " |      +-----+------------------------------------------------------------+\n",
      " |      |Alice|                                                          -1|\n",
      " |      |  Bob|                                                           1|\n",
      " |      +-----+------------------------------------------------------------+\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
